{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "52f3fba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "874011eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        # TODO: return output\n",
    "        pass\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        # TODO: update parameters and return input gradient\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "275bfed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Activation(Layer):\n",
    "    def __init__(self, activation, activation_prime):\n",
    "        self.activation = activation\n",
    "        self.activation_prime = activation_prime\n",
    "\n",
    "    def forward(self, input):\n",
    "        self.input = input\n",
    "        return self.activation(self.input)\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        print(\"outputgrad\",output_gradient,\"\\n\\n\\n\")\n",
    "        print(\"self.input derivate\",self.activation_prime(self.input),\"\\n\\n\\n\")\n",
    "        return output_gradient.T* self.activation_prime(self.input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "c5b22a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#solo relu Ã¨ fatta appositamente per adaw\n",
    "\n",
    "class Tanh(Activation):\n",
    "    def __init__(self):\n",
    "        def tanh(x):\n",
    "            return np.tanh(x)\n",
    "\n",
    "        def tanh_prime(x):\n",
    "            return 1 - np.tanh(x) ** 2\n",
    "\n",
    "        super().__init__(tanh, tanh_prime)\n",
    "        \n",
    "class Relu(Activation):\n",
    "    def __init__(self):\n",
    "        def relu(x):\n",
    "            return np.sum(np.maximum(0,x),axis=1)\n",
    "\n",
    "        def relu_prime(x):\n",
    "            drelu = np.zeros_like(x)\n",
    "            drelu[x > 0] = 1\n",
    "            return drelu\n",
    "\n",
    "        super().__init__(relu, relu_prime)\n",
    "\n",
    "class Sigmoid(Activation):\n",
    "    def __init__(self):\n",
    "        def sigmoid(x):\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "\n",
    "        def sigmoid_prime(x):\n",
    "            s = sigmoid(x)\n",
    "            return s * (1 - s)\n",
    "\n",
    "        super().__init__(sigmoid, sigmoid_prime)\n",
    "\n",
    "class Softmax(Layer):\n",
    "    def forward(self, input):\n",
    "        tmp = np.exp(input)\n",
    "        self.output = tmp / np.sum(tmp)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        # This version is faster than the one presented in the video\n",
    "        n = np.size(self.output)\n",
    "        return np.dot((np.identity(n) - self.output.T) * self.output, output_gradient)\n",
    "        # Original formula:\n",
    "        # tmp = np.tile(self.output, n)\n",
    "        # return np.dot(tmp * (np.identity(n) - np.transpose(tmp)), output_gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "6e5487f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "ae369a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#ADAW NEURAL NETWORK (x2*x1.T).T\n",
    "\n",
    "class Dense(Layer):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.weights = np.random.randn(output_size, input_size)\n",
    "        self.bias = np.random.randn(output_size, input_size)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        self.input = np.array(input)\n",
    "        return self.weights*self.input.T + self.bias\n",
    "\n",
    "    def backward(self, output_gradient, learning_rate):\n",
    "        weights_gradient = output_gradient* self.input.T\n",
    "        input_gradient = self.weights*output_gradient\n",
    "        self.weights -= learning_rate * weights_gradient\n",
    "        self.bias -= learning_rate * output_gradient\n",
    "        return input_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "d7d54a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean(np.power(y_true - y_pred, 2))\n",
    "\n",
    "def mse_prime(y_true, y_pred):\n",
    "    return 2 * (y_pred - y_true) / np.size(y_true)\n",
    "\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    return np.mean(-y_true * np.log(y_pred) - (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "def binary_cross_entropy_prime(y_true, y_pred):\n",
    "    return ((1 - y_true) / (1 - y_pred) - y_true / y_pred) / np.size(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "67828e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(network, input):\n",
    "    output = input\n",
    "    for layer in network:\n",
    "        output = layer.forward(output)\n",
    "        print(output)\n",
    "    print(\"\\n\");\n",
    "    return output\n",
    "\n",
    "def train(network, loss, loss_prime, x_train, y_train, epochs = 1000, learning_rate = 0.01, verbose = True):\n",
    "    print(f\"{network[0].weights} \\n\\n{network[0].bias}\\n\\n\")\n",
    "    print(f\"{network[2].weights} \\n\\n{network[2].bias}\\n\\n\")\n",
    "    for e in range(epochs):\n",
    "        error = 0\n",
    "        for x, y in zip(x_train, y_train):\n",
    "            # forward\n",
    "            output = predict(network, x)\n",
    "\n",
    "            # error\n",
    "            error += loss(y, output)\n",
    "\n",
    "            # backward\n",
    "            grad = loss_prime(y, output)\n",
    "            for layer in reversed(network):\n",
    "                print(grad,\" \\n\\n\")\n",
    "                grad = layer.backward(grad, learning_rate)\n",
    "                \n",
    "\n",
    "        error /= len(x_train)\n",
    "        if 1:#e>epochs-3:\n",
    "            print(f\"{e + 1}/{epochs}, error={error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "61b8eafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.51757918 -1.51240855]\n",
      " [-1.66061205 -1.61325811]\n",
      " [-0.94318089  0.02838684]] \n",
      "\n",
      "[[-0.52211793  1.41532847]\n",
      " [-0.33890173  0.09929261]\n",
      " [-0.33058418  0.16357674]]\n",
      "\n",
      "\n",
      "[[ 0.53336336 -0.61172847  0.26533905]] \n",
      "\n",
      "[[1.51545276 2.14121418 1.05417787]]\n",
      "\n",
      "\n",
      "[[-0.52211793  1.41532847]\n",
      " [-0.33890173  0.09929261]\n",
      " [-0.33058418  0.16357674]]\n",
      "[1.41532847 0.09929261 0.16357674]\n",
      "[[2.27033711 2.08047406 1.09758116]]\n",
      "[5.44839234]\n",
      "\n",
      "\n",
      "[[10.89678468]]  \n",
      "\n",
      "\n",
      "outputgrad [[10.89678468]] \n",
      "\n",
      "\n",
      "\n",
      "self.input derivate [[1. 1. 1.]] \n",
      "\n",
      "\n",
      "\n",
      "[[10.89678468 10.89678468 10.89678468]]  \n",
      "\n",
      "\n",
      "[[ 5.8119457  -6.66587347  2.89134254]]  \n",
      "\n",
      "\n",
      "outputgrad [[ 5.8119457  -6.66587347  2.89134254]] \n",
      "\n",
      "\n",
      "\n",
      "self.input derivate [[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]] \n",
      "\n",
      "\n",
      "\n",
      "[[ 0.          5.8119457 ]\n",
      " [-0.         -6.66587347]\n",
      " [ 0.          2.89134254]]  \n",
      "\n",
      "\n",
      "[[-0.52211793 -0.67827464]\n",
      " [-0.33890173 -0.84737816]\n",
      " [-0.33058418 -0.09717067]]\n",
      "[0. 0. 0.]\n",
      "[[ 0.42577429  1.05153571 -0.0355006 ]]\n",
      "[1.47731]\n",
      "\n",
      "\n",
      "[[0.95462001]]  \n",
      "\n",
      "\n",
      "outputgrad [[0.95462001]] \n",
      "\n",
      "\n",
      "\n",
      "self.input derivate [[1. 1. 0.]] \n",
      "\n",
      "\n",
      "\n",
      "[[0.95462001 0.95462001 0.        ]]  \n",
      "\n",
      "\n",
      "[[-0.9631062  -0.68725528  0.        ]]  \n",
      "\n",
      "\n",
      "outputgrad [[-0.9631062  -0.68725528  0.        ]] \n",
      "\n",
      "\n",
      "\n",
      "self.input derivate [[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]] \n",
      "\n",
      "\n",
      "\n",
      "[[-0. -0.]\n",
      " [-0. -0.]\n",
      " [ 0.  0.]]  \n",
      "\n",
      "\n",
      "[[-2.03969711  0.8341339 ]\n",
      " [-1.99951378  0.76587995]\n",
      " [-1.27376507 -0.12555752]]\n",
      "[0.8341339  0.76587995 0.        ]\n",
      "[[-0.51123673  0.40469721 -0.0355006 ]]\n",
      "[0.40469721]\n",
      "\n",
      "\n",
      "[[-1.19060558]]  \n",
      "\n",
      "\n",
      "outputgrad [[-1.19060558]] \n",
      "\n",
      "\n",
      "\n",
      "self.input derivate [[0. 1. 0.]] \n",
      "\n",
      "\n",
      "\n",
      "[[-0.         -1.19060558 -0.        ]]  \n",
      "\n",
      "\n",
      "[[ 0.          0.85714731 -0.        ]]  \n",
      "\n",
      "\n",
      "outputgrad [[ 0.          0.85714731 -0.        ]] \n",
      "\n",
      "\n",
      "\n",
      "self.input derivate [[0. 1.]\n",
      " [0. 1.]\n",
      " [0. 0.]] \n",
      "\n",
      "\n",
      "\n",
      "[[ 0.          0.        ]\n",
      " [ 0.          0.85714731]\n",
      " [-0.         -0.        ]]  \n",
      "\n",
      "\n",
      "[[-2.03969711 -0.67827464]\n",
      " [-1.99951378 -0.93309289]\n",
      " [-1.27376507 -0.09717067]]\n",
      "[0. 0. 0.]\n",
      "[[ 0.33031229  1.07513427 -0.0355006 ]]\n",
      "[1.40544656]\n",
      "\n",
      "\n",
      "[[2.81089312]]  \n",
      "\n",
      "\n",
      "outputgrad [[2.81089312]] \n",
      "\n",
      "\n",
      "\n",
      "self.input derivate [[1. 1. 0.]] \n",
      "\n",
      "\n",
      "\n",
      "[[2.81089312 2.81089312 0.        ]]  \n",
      "\n",
      "\n",
      "[[-2.83588084 -1.76731924  0.        ]]  \n",
      "\n",
      "\n",
      "outputgrad [[-2.83588084 -1.76731924  0.        ]] \n",
      "\n",
      "\n",
      "\n",
      "self.input derivate [[0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]] \n",
      "\n",
      "\n",
      "\n",
      "[[-0. -0.]\n",
      " [-0. -0.]\n",
      " [ 0.  0.]]  \n",
      "\n",
      "\n",
      "1/1, error=8.06061734501523\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = np.reshape([[0, 0], [0, 1], [1, 0], [1, 1]], (4, 2, 1))\n",
    "Y = np.reshape([[0], [1], [1], [0]], (4, 1, 1))\n",
    "\n",
    "network = [\n",
    "    Dense(2, 3),\n",
    "    Relu(),\n",
    "    Dense(3, 1),\n",
    "    Relu()\n",
    "]\n",
    "\n",
    "# train\n",
    "train(network, mse, mse_prime, X, Y, epochs=1, learning_rate=0.1)\n",
    "# for x, y in zip(X, Y):\n",
    "#     print(f\"{network[0].weights} \\n\\n{network[0].bias}\\n\\n\")\n",
    "#     output = predict(network, x)\n",
    "    \n",
    "# decision boundary plot\n",
    "# points = []\n",
    "# for x in np.linspace(0, 1, 20):\n",
    "#     for y in np.linspace(0, 1, 20):\n",
    "#         z = predict(network, [[x], [y]])\n",
    "#         points.append([x, y, z[0,0]])\n",
    "\n",
    "# points = np.array(points)\n",
    "# print(network[0].weights,network[0].bias,network[2].weights,network[2].bias)\n",
    "# fig = plt.figure()\n",
    "# ax = fig.add_subplot(111, projection=\"3d\")\n",
    "# ax.scatter(points[:, 0], points[:, 1], points[:, 2], c=points[:, 2], cmap=\"winter\")\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "# (self.weights.T*output_gradient).T\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "67d77c73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [2., 3.],\n",
       "       [4., 5.]])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = np.arange(6.0).reshape((3,2 ))\n",
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "b70f7ed9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.],\n",
       "       [1.],\n",
       "       [2.]])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = np.arange(3.0).reshape((3,1))\n",
    "x2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "1be90712",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0.],\n",
       "       [3., 4., 5.]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x2*x1.T).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "7fc1f44b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.],\n",
       "       [ 2.,  3.],\n",
       "       [ 8., 10.]])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2*x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8b417d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
